{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeoSXsVuzsTf"
   },
   "source": [
    "# Week 5 - Deep learning with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxq_lY1Wz2AK"
   },
   "source": [
    "Some preliminary set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "rK63fnkquGHY",
    "outputId": "8b81d4c9-045a-4eed-e0cf-59ac8858c30c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "data": {
      "text/plain": "Device: \u001B[31mcpu\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device: <span style=\"color: #800000; text-decoration-color: #800000\">cpu</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install ipywidgets rich seaborn torch datasets transformers tokenizers sentencepiece sacremoses --quiet\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import rich\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tokenizers\n",
    "import datasets\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# define the device to use\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "rich.print(f\"Device: [red]{DEVICE}\")\n",
    "\n",
    "# control verbosity\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# define support functions\n",
    "def load_glove_vectors(filename = \"glove.6B.300d.txt\") -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"Load the GloVe vectors. See: `https://github.com/stanfordnlp/GloVe`\"\"\"\n",
    "    path = Path(hf_hub_download(repo_id=\"stanfordnlp/glove\", filename=\"glove.6B.zip\"))\n",
    "    target_file = path.parent / filename\n",
    "    if not target_file.exists():\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path.parent)\n",
    "\n",
    "        if not target_file.exists():\n",
    "            print(f\"Available files:\")\n",
    "            for p in path.parent.iterdir():\n",
    "                print(p)\n",
    "            raise ValueError(f\"Target file `{target_file.name}` can't be found. Check if `{filename}` was properly downloaded.\")\n",
    "\n",
    "    # parse the vocabulary and the vectors\n",
    "    vocabulary = []\n",
    "    vectors = []\n",
    "    with open(target_file, \"r\") as f:\n",
    "        for l in tqdm(f.readlines(), desc=f\"Parsing {target_file.name}...\" ):\n",
    "            word, *vector = l.split()\n",
    "            vocabulary.append(word)\n",
    "            vectors.append(torch.tensor([float(v) for v in vector]))\n",
    "    vectors = torch.stack(vectors)\n",
    "    return vocabulary, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c0ac4f025e3e4f1f8139f32b6944c4eb",
      "2c24c3519f3d4b9fb74212e04c295375",
      "29e1fba88dc44a7490fc65cd9dc0eef1",
      "d3b630f54521489fa7da02ec47e9e220",
      "11a608d17f0041d7966e54b1379eb963",
      "9c4b9c55c6ac4582bc98acdbe774c87c",
      "e5fe5c0a9bbe4a41a4f56931bbafb41b",
      "6a3afe212a70491d8dacfc4ff7ba45e4",
      "971ddad710c6403db026dfe89176cec5",
      "661e8b0c7ab6495f93102bb2b0a23444",
      "2e2f47bcd0294757b30a734ea11fa959"
     ]
    },
    "id": "gtB1kzl_uGHm",
    "outputId": "72f885d7-1102-41b9-b606-64a0d20b1368"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing glove.6B.300d.txt...: 100%|██████████| 400001/400001 [00:53<00:00, 7487.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "glove_vocabulary: \u001B[33mtype\u001B[0m=\u001B[1m<\u001B[0m\u001B[1;95mclass\u001B[0m\u001B[39m \u001B[0m\u001B[32m'list'\u001B[0m\u001B[1m>\u001B[0m, \u001B[33mlength\u001B[0m=\u001B[1;36m400001\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">glove_vocabulary: <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400001</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "glove_vectors: \u001B[33mtype\u001B[0m=\u001B[1m<\u001B[0m\u001B[1;95mclass\u001B[0m\u001B[39m \u001B[0m\u001B[32m'torch.Tensor'\u001B[0m\u001B[1m>\u001B[0m, \u001B[33mshape\u001B[0m=\u001B[1;35mtorch\u001B[0m\u001B[1;35m.Size\u001B[0m\u001B[1m(\u001B[0m\u001B[1m[\u001B[0m\u001B[1;36m400001\u001B[0m, \u001B[1;36m300\u001B[0m\u001B[1m]\u001B[0m\u001B[1m)\u001B[0m, \u001B[33mdtype\u001B[0m=\u001B[35mtorch\u001B[0m.float32\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">glove_vectors: <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.Tensor'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400001</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.float32\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data for the later cells\n",
    "glove_vocabulary, glove_vectors = load_glove_vectors()\n",
    "rich.print(f\"glove_vocabulary: type={type(glove_vocabulary)}, length={len(glove_vocabulary)}\")\n",
    "rich.print(f\"glove_vectors: type={type(glove_vectors)}, shape={glove_vectors.shape}, dtype={glove_vectors.dtype}\")\n",
    "\n",
    "# add special tokens\n",
    "special_tokens = ['<|start|>', '<|unknown|>', '<|pad|>']\n",
    "glove_vocabulary = special_tokens + glove_vocabulary\n",
    "glove_vectors = torch.cat([torch.randn_like(glove_vectors[:len(special_tokens)]), glove_vectors])\n",
    "\n",
    "# tokenizer for GloVe\n",
    "glove_tokenizer = tokenizers.Tokenizer(tokenizers.models.WordLevel(vocab={v:i for i,v in enumerate(glove_vocabulary)}, unk_token=\"<|unknown|>\"))\n",
    "glove_tokenizer.normalizer = tokenizers.normalizers.BertNormalizer(strip_accents=False)\n",
    "glove_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKoXNCEfuGHr"
   },
   "source": [
    "# Language Modelling and Transformers\n",
    "\n",
    "___\n",
    "## Content\n",
    "\n",
    "* I. Text to vectors\n",
    "* II. Language models\n",
    "* III. Attention mechanism\n",
    "* IV. Transformers\n",
    "* V. Applications of Transformer-based language models\n",
    "\n",
    "\n",
    "___\n",
    "## Introduction\n",
    "\n",
    "Since its introduction ([\"Attention is All You Need\", Wasrani et al. (2016)](https://arxiv.org/abs/1706.03762)), Transformers have overtaken the field of Machine Learning. Initially applied to translation tasks, Transformers pre-trained on vast amounts of unlabelled data such as BERT and GPT have been acquired as central components in most of the modern natural language processing (NLP) systems. Transformers power question answering (QA) models, machine translation services, search engines and chat bots. Independently of the language applications, the Transformer is also a versatile neural architecture and, therefore, has found success outside the field of NLP. Transformers are rapidly being adopted in image processing ([\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", Dosovitskiy et al. (2021)](https://arxiv.org/abs/2010.11929)), in reinforcement learning ([\"A Generalist Agent\", Reed et al. (2022)](https://arxiv.org/abs/2205.06175)), video generation ([\"VideoGPT: Video Generation using VQ-VAE and Transformers\", Yan et al. (2021)](https://arxiv.org/abs/2104.10157)), and more. In the following sections, we will first introduce the basics of NLP (tokenization, token embeddings, language modelling), introduce the attention mechanism. In the second part, we will study the Transformer architecture and apply it to NLP tasks.\n",
    "\n",
    "___\n",
    "## I. Text to vectors\n",
    "\n",
    "In the previous labs, we have applied deep learning to processing images encoded as RGB pixels. We found that processing arrays of RGB pixels using convolutional neural network was effective. In NLP, other neural interfaces are required to enable plugging text into neural networks. Raw text cannot trivially be plugged-in neural networks. In this section we show how to convert text units or *tokens* into vectors and introduce the notion of text vector spaces.\n",
    "\n",
    "### I.a. Tokenization\n",
    "\n",
    "In [alphabetic languages](https://en.wikipedia.org/wiki/List_of_writing_systems), text can be decomposed into various types of units or *tokens*: characters, syllables, words or even sentences. Each tokenization system comes with vocabulary $\\mathcal{V}$ that references all known symbols. \n",
    "\n",
    "The choice of tokenizer is a tradeoff between the size of the vocabulary and the number of tokens required to encode a sentence. For instance, character-level tokenizers result in a smaller vocabulary size (only 128 character when using ASCII encoding) than other tokenizers. Word-based tokenizers encode text using fewer tokens than the other tokenizers but require a much larger vocabulary, which still might miss words seen at test time. Sub-words tokenizers such as [WordPiece](https://arxiv.org/abs/2012.15524) and [byte-pair encoding (BPE)](https://arxiv.org/abs/1508.07909) are a tradeoff between character-level and word-level encoding. They have progressively taken over the field as they provide two main advantages: (i) good tradeoff between vocabulary size and encoding length, (ii) open-ended vocabulary. \n",
    "\n",
    "Below we tokenize one sentence using word-level, character-level and sub-word-level tokenizers. In each case, the output corresponds to a sequence of indexes corresponding to the position of the given token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPEQDuq4uGHz",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Input sentence: \u001B[1;34m`It is jubilating to see how élégant my horse has became`\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input sentence: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">`It is jubilating to see how élégant my horse has became`</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[31mWord-level\u001B[0m: sentence converted into \u001B[1;36m11\u001B[0m tokens \u001B[1m(\u001B[0mvocabulary: \u001B[1;36m400004\u001B[0m tokens\u001B[1m)\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Word-level</span>: sentence converted into <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> tokens <span style=\"font-weight: bold\">(</span>vocabulary: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400004</span> tokens<span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Tokens:\n\u001B[1m[\u001B[0m\u001B[32m'it'\u001B[0m, \u001B[32m'is'\u001B[0m, \u001B[32m'\u001B[0m\u001B[32m<\u001B[0m\u001B[32m|unknown|\u001B[0m\u001B[32m>\u001B[0m\u001B[32m'\u001B[0m, \u001B[32m'to'\u001B[0m, \u001B[32m'see'\u001B[0m, \u001B[32m'how'\u001B[0m, \u001B[32m'\u001B[0m\u001B[32m<\u001B[0m\u001B[32m|unknown|\u001B[0m\u001B[32m>\u001B[0m\u001B[32m'\u001B[0m, \u001B[32m'my'\u001B[0m, \u001B[32m'horse'\u001B[0m, \u001B[32m'has'\u001B[0m, \u001B[32m'became'\u001B[0m\u001B[1m]\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Tokens:\n<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|unknown|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'see'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'how'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|unknown|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'my'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'horse'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'has'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'became'</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Token ids:\n\u001B[1m[\u001B[0m\u001B[1;36m23\u001B[0m, \u001B[1;36m17\u001B[0m, \u001B[1;36m1\u001B[0m, \u001B[1;36m7\u001B[0m, \u001B[1;36m256\u001B[0m, \u001B[1;36m200\u001B[0m, \u001B[1;36m1\u001B[0m, \u001B[1;36m195\u001B[0m, \u001B[1;36m2870\u001B[0m, \u001B[1;36m34\u001B[0m, \u001B[1;36m305\u001B[0m\u001B[1m]\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Token ids:\n<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">195</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2870</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">305</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[31mWordPiece\u001B[0m: sentence converted into \u001B[1;36m15\u001B[0m tokens \u001B[1m(\u001B[0mvocabulary: \u001B[1;36m28996\u001B[0m tokens\u001B[1m)\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">WordPiece</span>: sentence converted into <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> tokens <span style=\"font-weight: bold\">(</span>vocabulary: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28996</span> tokens<span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example sentence with rare English words and non-english words\n",
    "sentence = \"It is jubilating to see how élégant my horse has became\"\n",
    "rich.print(f\"Input sentence: [bold blue]`{sentence}`\")\n",
    "\n",
    "# Define multiple tokenizers\n",
    "tokenizer_ids = {\n",
    "    \"Word-level\": glove_tokenizer,\n",
    "    \"WordPiece\": \"bert-base-cased\",\n",
    "    \"BPE\": \"distilgpt2\",\n",
    "    \"Character-level\":  \"google/byt5-small\",\n",
    "    }\n",
    "\n",
    "# iterate through the tokenizers and decode the input sentences\n",
    "for tokenizer_name, tokenizer in tokenizer_ids.items():\n",
    "    # intialize the tokenizer (either)\n",
    "    if isinstance(tokenizer, str):\n",
    "        # init a `transformers.PreTrainedTokenizerFast`\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer)\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "    else:\n",
    "        # use the provided `tokenizers.Tokenizer``\n",
    "        vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "    # Tokenize\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    if isinstance(token_ids, tokenizers.Encoding):\n",
    "        token_ids = token_ids.ids\n",
    "\n",
    "    # Report\n",
    "    rich.print(f\"[red]{tokenizer_name}[/red]: sentence converted into {len(token_ids)} tokens (vocabulary: {vocab_size} tokens)\")\n",
    "    rich.print(f\"Tokens:\\n{[tokenizer.decode([t]) for t in token_ids]}\")\n",
    "    rich.print(f\"Token ids:\\n{[t for t in token_ids]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNH3dQ3VuGH1"
   },
   "source": [
    "### I.b Embeddings\n",
    "\n",
    "A tokenizer transforms fragments of text into list of integers that maps a vocabulary. We assign one vector of dimension $d$ to each item in the vocabulary of size $N_\\mathcal{V}$, this results in a matrix $E$ of dimension ${N_\\mathcal{V} \\times d}$. Converting a fragment of text into a sequence of vector representations can be done by tokenizing the text, and then looking up the embedding vector for each token, which is equivalent to *one-hot encoding* the tokens and performing a matrix multiplication using $E$. Given $\\mathbf{t}_1, \\ldots, \\mathbf{t}_L$ the sequence of one-hot encoded tokens, this is equivalent to\n",
    "$$\n",
    "\\mathbf{w}_i = E  \\mathbf{t}_i ,\n",
    "$$\n",
    "In the code below, we encode the sentence `Hellow world!` using a BPE tokenizer and a set of embedding of dimension `hdim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKD5CuNLuGH2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "hdim = 5 # embedding dimension\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\") # tokenizer\n",
    "sentence = \"Hello World!\" # input text\n",
    "embeddings = torch.randn((tokenizer.vocab_size, hdim)) # embedding matrix\n",
    "rich.print(f\"Embeddings (shape): {embeddings.shape}\")\n",
    "token_ids = tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0]\n",
    "rich.print(f\"Tokens ids (shape): {token_ids.shape}\")\n",
    "vectors =  torch.nn.functional.one_hot(token_ids, tokenizer.vocab_size).float() @ embeddings # equivalent to a `nn.Linear` layer\n",
    "rich.print(f\"Vectors (shape): {vectors.shape}\")\n",
    "rich.print(f\"List of tokens and their corresponding vectors:\")\n",
    "for t,v in zip(token_ids, vectors):\n",
    "    token_info = f\"[blue]{tokenizer.decode(t):5}[/blue] (token id: {t:4})\"\n",
    "    rich.print(f\" * {token_info} -> {v}\")\n",
    "\n",
    "# NB: in practice, we use the simpler interface `torch.nn.Embedding``\n",
    "# embeddings = torch.nn.Embedding(tokenizer.vocab_size, hdim)\n",
    "# vectors = embeddings(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ9GcEgOuGH4"
   },
   "source": [
    "### I.c Word vectors\n",
    "\n",
    "<img src=\"https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/word2vec-intuition.png?raw=1\" alt=\"Word2vec: translations in the vector spaces correspond to linguistic concepts (gender, verb tense, association between concepts)\" width=\"800px\"/>\n",
    "\n",
    "\n",
    "Word2vec ([\"Efficient Estimation of Word Representations in Vector Space\", Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781)) converts words into vector representations, which are learned using the Skip-Gram algorithm. Intuitively, The algorithm is based on the idea that words that appear together are related to each other.\n",
    "\n",
    "The word vector space allows to use the inner product to compare words, and arithmetic operations to manipulate word representations. For instance, in a well-defined word vector space, the concept \"king\" can be translated into \"queen\" by applying a linear transformation and the vector `vec(\"captial\") - vec(\"country\")` was found to correspond to the relative concept `\"capital city of a country\"` (see above illustration (*Image credits: https://www.tensorflow.org/tutorials/word2vec*)).\n",
    "\n",
    "\n",
    "**Experiment** In the first cells, we have downloaded the [GloVe word vectors](ttps://github.com/stanfordnlp/GloVe) from [\"GloVe: Global Vectors for Word Representation\", Jeffrey Pennington et al. (2014)](https://arxiv.org/abs/1902.11004). GloVe are trained using a Skip-Gram objective on a large collection of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQHCpsoXuGH7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def word2vec(\n",
    "        word: str,\n",
    "        vocabulary:List[str],\n",
    "        vectors: torch.Tensor\n",
    "    ) -> Optional[torch.Tensor]:\n",
    "    \"\"\"Convert a word into a vector\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in vocabulary:\n",
    "        word_idx = vocabulary.index(word)\n",
    "        return vectors[word_idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def vec2words(\n",
    "        vec: torch.Tensor,\n",
    "        k=5,\n",
    "        *,\n",
    "        vocabulary:List[str],\n",
    "        vectors: torch.Tensor,\n",
    "        exclude_vecs: List[torch.Tensor] = None,\n",
    "    ) -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"Retrieve the nearest word neighbours for an input vector\"\"\"\n",
    "\n",
    "    # compute the similarity between `vec`and all the vectors in `glove_vectors`\n",
    "    similarity = vectors @ vec\n",
    "\n",
    "    # potentially filter out some vocabulary entries\n",
    "    if exclude_vecs is not None and len(exclude_vecs):\n",
    "        mask = None\n",
    "        for e in exclude_vecs:\n",
    "            mask_ = (vectors == e[None, :]).all(dim=1)\n",
    "            if mask is None:\n",
    "                mask = mask_\n",
    "            else:\n",
    "                mask |= mask_\n",
    "        similarity.masked_fill_(mask=mask, value=-math.inf)\n",
    "\n",
    "    # return the ids of the nearesrt neighbours given the similarity\n",
    "    nearest_neighbour_ids = torch.argsort(-similarity)[:k]\n",
    "\n",
    "    # retrieve the corresponding words in the `vocabulary``\n",
    "    return [vocabulary[idx] for idx in nearest_neighbour_ids], similarity[nearest_neighbour_ids]\n",
    "\n",
    "# register the vocab and vectors args\n",
    "glove_args = {'vocabulary':glove_vocabulary, 'vectors':glove_vectors}\n",
    "\n",
    "# Nearest neighbours\n",
    "rich.print(\"[red]Nearest neighbour search:\")\n",
    "for word in [\"king\", \"queen\", \"dog\", \"France\"]:\n",
    "    rich.print(f'Nearest neighbours of the word \"{word}\":')\n",
    "    word_vec = word2vec(word, **glove_args)\n",
    "    words, similarities = vec2words(word_vec, k=5, **glove_args, exclude_vecs=[word_vec])\n",
    "    rich.print(f\"Words: {words}\")\n",
    "    rich.print(f\"Similarities: {similarities}\")\n",
    "\n",
    "# Word analogies\n",
    "rich.print(\"\\n[red]Vector arithmetic:\")\n",
    "cases = [\n",
    "    [(\"+\", \"king\"), (\"-\", \"man\"), (\"+\", \"woman\")],\n",
    "    [(\"+\", \"denmark\"), (\"-\", \"france\"), (\"+\", \"paris\")],\n",
    "    [(\"+\", \"pakistan\"), (\"-\", \"belgium\"), (\"+\", \"brussels\")],\n",
    "]\n",
    "for operations in cases:\n",
    "    # current location in the vector space\n",
    "    location = 0\n",
    "    rich.print(f\"Vector Translation: [blue]0 {' '.join(f'{d} {v}' for d,v in operations)} = \")\n",
    "    for sign, word in operations:\n",
    "        # retrieve the `vec(word)``\n",
    "        vec = word2vec(word, **glove_args)\n",
    "        if vec is None:\n",
    "            raise ValueError(f\"Unknown word `{word}`\")\n",
    "\n",
    "        # parse the direction (+/-)\n",
    "        direction = {\"+\": 1, \"-\": -1}[sign]\n",
    "\n",
    "        # apply the vector transform to the current location\n",
    "        location  +=  direction * vec\n",
    "\n",
    "    # return the nearest neighbours of the end location\n",
    "    exclude_list = [word2vec(w, **glove_args) for _, w in operations]\n",
    "    words, similarities = vec2words(location, k=5, exclude_vecs=exclude_list, **glove_args)\n",
    "    rich.print(f\"Words: {words}\")\n",
    "    rich.print(f\"Similarities: {similarities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZnUwuv0uGIA"
   },
   "source": [
    "**Beyond word2vec**  The Skip-Gram model allows to learn meaningful word prepresentations and arithmetic in the resulting vector space allow manipulating concepts. Ultimately, we are interested in learning representations that represent larger text fragments such as sentences, paragraphs or documents. Doing so require combining multiple vectors, which can be done by exploiting arithmetic in the vector space, or by combining word-vectors using deep neural networks, such as Transformers!\n",
    "\n",
    "___\n",
    "## II. Language models\n",
    "\n",
    "We have seen how to encode text into sequences of tokens, seen how to convert tokens into vectors using a set of embeddings and experimented with a GloVe word vector space. In this section we will see how to model text at the sentence, pragraph or even document level using language models.\n",
    "\n",
    "### II.a Language Modelling\n",
    "\n",
    "*Figure: Left-to-right language models*\n",
    "![Autoregressive left-to-right language model](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/ar-lm.png?raw=1)\n",
    "\n",
    "**Autoregressive factorization** Language models aim at grasping the underlying linguistic structure of a fragment of text: whereas word vectors model words independently of each others, a language model tracks the grammatical and semantic relationships between word tokens. Given a piece of text encoded into tokens $\\mathbf{w}_{1:T} = [\\mathbf{w_1}, \\ldots, \\mathbf{w}_T]$ a *left-to-right* language model describes $\\mathbf{w}_{1:T}$ with the following factorization:\n",
    "$$\n",
    " p_\\theta(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t}) \\ ,\n",
    "$$\n",
    "where $\\theta$ is a model parameter. The above *autoregressive* factorization describes a *recursive* function $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$, which is shared across all the time steps. In the above figure, we represent a left-to-right language model with dependencies represented by arrows for fixed steps $t=3$ and $t=4$. Because of this choice of factorization, a language model defines a graphical model where each step $t$ depends on all the previous steps $<t$ and the conditional $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ models the dependendies between the context $\\mathbf{w}_{<t}$ and the variable $\\mathbf{w}_t$.\n",
    "\n",
    "**Other factorizations** Autoregressive models are not required to adopt a left-to-right factorization and other forms of factorizations can be implemented (right-to-left or arbitrary permutations). See [\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\", Yang et al. (2019)](https://arxiv.org/abs/1906.08237) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5dhjnjNuGID"
   },
   "source": [
    "*Figure: Categorical distribution over the possible next tokens given the context*\n",
    "![Categorical distribution](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/categorical-dist.png?raw=1)\n",
    "\n",
    "**Distribution of the possible next tokens** The distribution $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ is a categorical distribution defined on the whole token vocabulary $\\mathcal{v}$. Without loss of generality, we denote $f_\\theta(\\mathbf{w}_t, \\mathbf{w}_{<t})$ the function with parameter $\\theta$ that parametrize:\n",
    "$$\n",
    "p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t}) = \\frac{\\exp f_\\theta(\\mathbf{w}_t, \\mathbf{w}_{<t})}{\\sum_{\\mathbf{v} \\in \\mathcal{V}} \\exp f_\\theta(\\mathbf{v}, \\mathbf{w}_{<t})} \\ .\n",
    "$$\n",
    "The distribution $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ assigns a probability for each token in the vocabulary to appear right after the context $\\mathbf{w}_{<t}$. As pictured in the above figure, many completion are possibles under the model $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$.\n",
    "\n",
    "**Language models learn contextual representations** Assigning a probability to each word in the vocabulary requires learning abstract representations of the context $\\mathbf{w}_{<t}$. For instance, in the horse example, predicting the word \"fast\" will be easier to predict if some knowledge of the grammatical rules and common sense is acquired. In this example example, the model needs to learn that $\\mathbf{w}_4$ must be an adjective, and that this adjective can be attributed to a horse. Therefore, the function $f_\\theta$ must acquire a non-trivial representation of the context to make sensible token predictions$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjOlZ-wSuGIE"
   },
   "source": [
    "**Sampling** At each step $t$, the $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ implements a categorical distribution defined on the vocabulary $\\mathcal{V}$. Sampling or *generating* text can by iteratively sampling tokens, as showed in the pseudo-code bellow:\n",
    "```python\n",
    "ws = [] # placeholder for all the samples w_t\n",
    "for t in range(T):\n",
    "    wt_logits = f(ws, theta) # logits of p(w_t | w_{<t})\n",
    "    wt = Categorical(logits=wt_logits).sample() # sampled w_t\n",
    "    ws.append(wt) # append the new sample to be used as input in the next step\n",
    "```\n",
    "Sampling is often expensive, as it requires one evaluation of the function $f_\\theta$ for each step and cannot be trivially parallelized.\n",
    "\n",
    "**Training** As long as the transition function $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ is differentiable (i.e., using neural networks), a language model can be  trained via maximum likelihood, e.g. maximizing the log-likelihood with the loss:\n",
    "$$\n",
    "L = - \\log p_\\theta(\\mathbf{w}_{1:T}) = - \\sum_{t=1}^T \\log p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})\n",
    "$$\n",
    "Each term $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ can be evaluated using the observed variables $\\mathbf{w}_t$ and $\\mathbf{w}_{<t}$ (no sampling is required) and thus training of auto-regressive models is fast when the evaluation of $f_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ can be parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH7KUquguGIE"
   },
   "source": [
    "*Figure: Bidirectional language models*\n",
    "![Masked language model](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/masked-lm.png?raw=1)\n",
    "\n",
    "**Bidirectional and masked language models** Autoregressive language models learn to predict a token $\\mathbf{w}_t$ given the context up to the step $t-1$. One can also use a [pseudo likelihood](https://en.wikipedia.org/wiki/Pseudolikelihood), where $\\mathbf{w}_t$ is not only conditioned on the preceeding tokens $\\mathbf{w}_{<t}$, but also on the next tokens $\\mathbf{w}_{>t}$. This defines a bidirectional language model, which factorizes as\n",
    "$$\n",
    "L_\\theta(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{-t}) \\ ,\n",
    "$$\n",
    "where $\\mathbf{w}_{-t}$ represent the set of tokens $\\mathbf{w}_{1:T} \\backslash \\{ \\mathbf{w}_t \\}$. We call it pseudo because this likelihood is not forming a valid distribution (because the graph formed by $\\mathbf{w}_{1:T}$ is not a directed acyclic graph (a DAG)).  Bidirectional language models such as [ELMo (\"Deep contextualized word representations\", Peters et al. (2018))](https://arxiv.org/abs/1802.05365), learn token representation contextualized on the whole context.\n",
    "\n",
    "In the case, of bidirectional language models, the context $\\mathbf{w}_{-t}$ corresponds to the whole sequence of tokens with the predicted element masked out. It is possible to generalize the bidirectional factorization to masking out one or more tokens. In that case, we consider a model $p_\\theta(\\mathbf{w}_m \\mid \\mathbf{w}_{-m})$ where $m$ is a set of indices of the tokens being predicted and $-m$ is the set of the other tokens. This is notably the approach adopted in [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", Delvin et al. (2018)](https://arxiv.org/abs/1810.04805)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQT5jAR5uGIF"
   },
   "source": [
    "### II.b Recurrent Neural Networks\n",
    "\n",
    "*Figure: Left-to-right recurrent neural network. We highlight the information flowing from the context \"My horse is\" to the predicted word \"very\".*\n",
    "![Recurrent Neural Network](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/recurrent-lm-activated.png?raw=1)\n",
    "\n",
    "**Recurrent neural networks (RNNs)** implement a recursive function $f_\\theta$ using neural networks, which makes them a particularly good fit for sequential data. In the general setting, RNNs model the acquired knowledge at time $t$ using an additional variable $\\mathbf{h}_t$ of dimension $d_h$ (*hidden state*). The hidden state at step $t-1$ is updated with the information extracted from the observation $\\mathbf{w}_t$ using a function\n",
    "$$\n",
    "h_\\theta: (\\mathbf{w}_{t}, \\mathbf{h}_{t-1}) \\rightarrow \\mathbf{h}_{t} \\ ,\n",
    "$$\n",
    "which can be imlemented using an arbitrary neural network that takes the tuple $(\\mathbf{w}_{t}, \\mathbf{h}_t)$ as input and returns a new hidden state $\\mathbf{h}_{t+1}$. RRNs can be applied to parametrize language models by projecting the hidden state $\\mathbf{t}$ into the vocabulary space using a projection matrix $\\mathbf{F} \\in \\mathcal{R}^{V \\times d_h}$. This results in parameterizing the transition distribution as\n",
    "$$\n",
    "p_\\theta(\\cdot \\mid \\mathbf{w}_{<t}) = \\mathrm{Softmax}( \\mathbf{h}_t \\mathbf{F}^T)\n",
    "$$\n",
    "In the above figure, we showcase how a standard RNN can be applied to implement a left-to-right language model, and annotated the diagramm with the function $h_\\theta(\\mathbf{w}_{t}, \\mathbf{h}_{t-1)})$ and the projection matrix $\\mathbf{F}$.\n",
    "\n",
    "**Long Short-Term Memory (LSTM) networks** A standard RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the [Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber. (1997))](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. RNNs coupled with gated mechanisms are less prone to the problem of vanishing gradients, and can therefore model dependencies over longer number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qGgbRBcuGIF"
   },
   "source": [
    "*Figure: bi-directional recurrent neural network. We highlight the information flowing from the context \"My horse is\" to the predicted word \"very\" (left-to-right), and the information flowing from the context \"fast\" (right-to-left).*\n",
    "![Recurrent Neural Network](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/bidirectional-lm-activated.png?raw=1)\n",
    "\n",
    "**Bi-directional recurrent neural networks** Using two RNNs running in reverse direction allows building bidirectional language models. The distribution $p_\\theta(\\mathbf{x}_t \\mid \\mathbf{x}_{-t})$ can be parameterized as\n",
    "$$\n",
    "p_\\theta( \\cdot \\mid \\mathbf{x}_{-t}) = \\mathrm{Softmax}((\\mathbf{h}^\\mathrm{forward}_t + \\mathbf{h}^\\mathrm{reverse}_t) F^T) \\ ,\n",
    "$$\n",
    "where the hidden state  $\\mathbf{h}^\\mathrm{bi}_t = \\mathbf{h}^\\mathrm{forward}_t + \\mathbf{h}^\\mathrm{reverse}_t$ defines hidden state contextualized on all the tokens but $\\mathbf{w}_t$. \n",
    "\n",
    "This is the strategy adopted by [ELMo (\"Deep contextualized word representations\", Peters et al. (2018))](https://arxiv.org/abs/1802.05365), which popularized learning deep contextualized representations as a pre-training step, and at the samd time, started a [tradition of naming models after Seame Street characters](https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbzQlrGXuGIF"
   },
   "source": [
    "**Experiment: train your own LSTM language model**\n",
    "\n",
    "> **NB**  *training on CPU is very slow. If you don't have access to a GPU, it will be difficult to train a model that generate acceptable samples. In the end of the notebook, we will use pre-trained models directly, so feel free to skip this experiment. **You still nee to implement the loss in the training loop***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzeY47_JuGIG",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_dataset_size = 1000 # let's use a small subset for now,\n",
    "max_seq_size = 10 # and very short sequences\n",
    "\n",
    "# load and tokenizer the dataset\n",
    "def batch_tokenize(batch: List[Dict[str, Any]], max_length=max_seq_size, tokenizer: tokenizers.Tokenizer = None, key:str=\"text\") -> torch.Tensor:\n",
    "    texts = batch[key]\n",
    "    encodings = tokenizer.encode_batch(texts)\n",
    "    return {\"token_ids\": [x.ids[:max_length] for x in encodings]}\n",
    "\n",
    "# load AG News, take a subset of `max_dataset_size` rows and tokenize\n",
    "dataset = datasets.load_dataset(\"ag_news\")\n",
    "dataset = datasets.DatasetDict({split: dset.select(range(max_dataset_size)) if len(dset) > max_dataset_size else dset for split, dset in dataset.items()})\n",
    "dataset = dataset.map(partial(batch_tokenize, tokenizer=glove_tokenizer), batched=True, num_proc=2, batch_size=10)\n",
    "rich.print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x4tdP88uGIH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class RNNLM(torch.nn.Module):\n",
    "    \"\"\"A simple implementation of a language model using RNNs.\"\"\"\n",
    "    def __init__(self, vectors:torch.Tensor):\n",
    "        super().__init__()\n",
    "        # register the embeddings\n",
    "        self.embeddings = torch.nn.Embedding(*glove_vectors.shape)\n",
    "        self.embeddings.weight.data = glove_vectors\n",
    "\n",
    "        # register the LSTM\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size=glove_vectors.shape[1],\n",
    "            hidden_size=glove_vectors.shape[1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # project the output of the LSTM (hidden state) back to the vocabulary space\n",
    "        self.proj = nn.Linear(glove_vectors.shape[1], glove_vectors.shape[0], bias=False)\n",
    "        # init the projection using the embeddings weights\n",
    "        self.proj.weight.data = glove_vectors\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, retain_ws:bool=False) -> torch.Tensor:\n",
    "        # convert the tokens into vectors\n",
    "        ws = self.embeddings(token_ids)\n",
    "\n",
    "        # store the word vectors for debugging\n",
    "        if retain_ws:\n",
    "          ws.retain_grad()\n",
    "          self.ws = ws\n",
    "\n",
    "        # shift the input `ws` right\n",
    "        w0 = torch.zeros((ws.shape[0], 1, self.embeddings.weight.shape[1]),\n",
    "                         device=self.embeddings.weight.device, dtype=torch.long)\n",
    "        ws_shifted = torch.cat([w0, ws[:, :-1]], dim=1)\n",
    "\n",
    "        # call the RNN: w_{-1:T-1} -> h{1:T}\n",
    "        hidden_states, _ = self.rnn(ws_shifted) # ws_shifted\n",
    "\n",
    "        # project the hidden state to the vocabulary space\n",
    "        logits = self.proj(hidden_states)\n",
    "        return logits\n",
    "\n",
    "    def sample(\n",
    "            self,\n",
    "            batch_size:int=1,\n",
    "            num_steps:int=10,\n",
    "            temperature: float=1.0,\n",
    "            prevent_repetitions: bool=False\n",
    "        ):\n",
    "        token_ids = torch.empty((batch_size, 0), device=self.embeddings.weight.device, dtype=torch.long)\n",
    "        for t in tqdm(range(num_steps), desc=f\"Sampling {num_steps} steps..\"):\n",
    "            logits = self.forward(token_ids)\n",
    "            logits_t = logits[:, -1:] / temperature\n",
    "            if prevent_repetitions and t > 0:\n",
    "                # mask the last generated tokens to avoid repetitions\n",
    "                logits_t.scatter_(index=token_ids[:,-1:, None], dim=2, value=-math.inf)\n",
    "            p_wt = torch.distributions.Categorical(logits=logits_t)\n",
    "            tokens_t = p_wt.sample()\n",
    "            token_ids = torch.cat([token_ids, tokens_t], dim=1)\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# init RNN initialized from GloVe vectors\n",
    "# delete the checkpoint if you get `PytorchStreamReader` error\n",
    "checkpoint_file = Path(\"rrn-lm.ckpt\")\n",
    "rnn = RNNLM(glove_vectors)\n",
    "if checkpoint_file.exists():\n",
    "    # checkpoint_file.unlink() # delete the checkpoint by un-commenting this line\n",
    "    rnn.load_state_dict(torch.load(checkpoint_file, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h9uwSmzuOdZ"
   },
   "source": [
    "**Testing** Let's make sure the autoregressive constrains are enforced ($\\mathbf{h}_t$ only depends on $\\mathbf{w}_{<t}$). We take differentiate a loss that depends only on a step $t = t'$ for each element $t'$ of the input batch and visualize the gradients with regards to the input word vectors (right after the embedding layers) $\\mathbf{w}_{1:T}$. The gradient map tells use which input positions are influencing the differentiated output position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_EeZagOuOde",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Test whether the autoregressive constrains (h_t only depends on w_{<t}) is enforced\n",
    "rnn.zero_grad()\n",
    "# get dummy token ids\n",
    "token_ids = torch.arange(0, 10)\n",
    "token_ids = token_ids[None].repeat(10, 1)\n",
    "# run through the RNN\n",
    "logits = rnn(token_ids, retain_ws=True)\n",
    "\n",
    "# compute a loss for a which depends only on step `t=i`\n",
    "loss_locations = torch.arange(0, 10)[:, None, None].expand(10, 1, logits.shape[-1])\n",
    "loss = logits.gather(index=loss_locations, dim=1).mean()\n",
    "\n",
    "# backward pass and retrieve the gradients with respect to the word vectors w_{1:T}\n",
    "loss.backward()\n",
    "grad_magnitude = rnn.ws.grad.norm(dim=2)\n",
    "rnn.ws = None\n",
    "\n",
    "# visualize the gradient\n",
    "grad_magnitude[grad_magnitude==0] = -math.inf # the the gradient that are exactly zero to -inf for the sake of visualization\n",
    "grad_magnitude = grad_magnitude.detach().cpu().numpy()\n",
    "plt.imshow(grad_magnitude, sns.color_palette(\"viridis\", as_cmap=True))\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.xlabel(\"$t$ (input)\")\n",
    "plt.ylabel(\"$t'$ (loss)\")\n",
    "plt.title(\"Magnitude of the gradient w.r.t. $\\mathbf{w}_{1:T}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKm2KVC1E0Zh"
   },
   "source": [
    "**Exercise 1**: Explain in your own words what the plot shows. How would it look if we had a leakage of information from the future to the present?\n",
    "\n",
    "> We see that the input have the highest magnitude on the output just after, so that input 1 has highest magnitude on 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL-deg8-uOdf"
   },
   "source": [
    "**Exercise 2**: Implement the loss of the RNN language model.\n",
    "\n",
    "> answer in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK_EllpwuGIH",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use Adam optimizer\n",
    "optimiser = torch.optim.AdamW(rnn.parameters(), lr=1e-3)\n",
    "\n",
    "# define a data loader to iterate the dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset['train'],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=list,\n",
    ")\n",
    "\n",
    "# convert token_ids to tensors\n",
    "def make_batch(batch: List[Dict[str, Any]], max_length=24, key:str=\"token_ids\") -> torch.Tensor:\n",
    "    \"\"\"Collate the `token_ids` into a single tensor, pad to minimum length.\"\"\"\n",
    "    token_ids = [b[key] for b in batch]\n",
    "    max_length = min(max_length, *(len(x) for x in token_ids))\n",
    "    token_ids = [torch.tensor(x) for x in token_ids]\n",
    "    return torch.stack([x[:max_length] for x in token_ids])\n",
    "\n",
    "# If you don't have enough GPU memory, decrease the batch size, potentially along with the learning rate.\n",
    "rnn = rnn.to(DEVICE)\n",
    "num_steps = 500 # 5_000\n",
    "step = 0\n",
    "epoch = 0\n",
    "with tqdm(total=num_steps) as pbar:\n",
    "    while step < num_steps:\n",
    "        for batch in train_loader:\n",
    "            # concatenate the `token_ids``\n",
    "            batch_token_ids = make_batch(batch)\n",
    "            batch_token_ids = batch_token_ids.to(DEVICE)\n",
    "\n",
    "            # forward through the model\n",
    "            optimiser.zero_grad()\n",
    "            batch_logits = rnn(batch_token_ids)\n",
    "\n",
    "            # compute the loss (negative log-likelihood)\n",
    "            p_ws = torch.distributions.Categorical(logits=batch_logits) \n",
    "\n",
    "            # Exercise: write the loss of the RNN language model\n",
    "            # hint: check the doc https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "            # NB: even with the right loss, training is slow and the generated samples won't be very good.\n",
    "            loss = -torch.sum(p_ws.logits) # <- YOUR CODE HERE\n",
    "\n",
    "            # backward and optimize\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Report\n",
    "            if step % 5 ==0 :\n",
    "                loss = loss.detach().cpu()\n",
    "                pbar.set_description(f\"epoch={epoch}, step={step}, loss={loss:.1f}\")\n",
    "\n",
    "            # save checkpoint\n",
    "            if step % 50 ==0 :\n",
    "                torch.save(rnn.state_dict(), checkpoint_file)\n",
    "            if step >= num_steps:\n",
    "                break\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uftZSXKLuGII",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# sample the RNN language model\n",
    "with torch.no_grad():\n",
    "    sample = rnn.sample(num_steps=10, batch_size=10, temperature=0.5, prevent_repetitions=True)\n",
    "    rich.print(glove_tokenizer.decode_batch(sample.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N1ntVHquGII",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_ = rnn.cpu()\n",
    "# free-up memory if needed: delete the RNN model\n",
    "# del rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH3nKK-cuOdy"
   },
   "source": [
    "**Exercise 3**: What would happen if the inputs of the RNN were not shifted to the right (in sample in the RNNLM class)?  \n",
    "\n",
    "> The input is now affecting the same output timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gSqXyp5uGIJ"
   },
   "source": [
    "___\n",
    "## III. Attention mechanism and Transformers\n",
    "\n",
    "\n",
    "The attention mechanism was first introduce in machine learning for machine translation tasks [(\"Neural Machine Translation by Jointly Learning to Align and Translate\", Bahdanau et al. (2014))](https://arxiv.org/abs/1409.0473). Translation is a sequence-to-sequence problem which goal is to generate a translation of a source text. The attention mechanism was introduced to allow *attenting* the whole source text at any of the generation steps. We implement attention with the softmax function because it is a differential version of a hard zero-one attention.\n",
    "\n",
    "In this section, we will introduce the *scaled dot-product* self-attention mechanism and the Transformer architecture [(\"Attention is All You Need\", Wasrani et al. (2016))](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "### III.a Attention mechanism\n",
    "\n",
    "Attention has become a very important concept in deep learning beginning with [\"Neural Machine Translation by Jointly Learning to Align and Translate\", Badanau et. al. (2015)](https://arxiv.org/abs/1409.0473). The idea key idea in that paper is that when you translate a sentence from for example German to English then it is a good is helpful for the model when it generates  \n",
    "\n",
    "We define three variables:\n",
    "1. The query $\\mathbf{Q} = [\\mathbf{q}_i, \\ldots \\mathbf{q}_{T_\\mathbf{Q}}] \\in \\mathcal{R}^{T_\\mathbf{Q} \\times h_i}$, a sequence of vectors of length $T_\\mathbf{Q}$ and vector dimension $h_i$.\n",
    "1. The keys $\\mathbf{K} = [\\mathbf{k}_1, \\ldots \\mathbf{k}_{T_{\\mathbf{K}\\mathbf{V}}}] \\in \\mathcal{R}^{T_{\\mathbf{K}\\mathbf{V}} \\times h_i}$, a sequence of vectors of length $T_{\\mathbf{K}\\mathbf{V}}$ and vector dimension $h_i$.\n",
    "1. The values $\\mathbf{V} = [\\mathbf{v}_1, \\ldots \\mathbf{k}_{T_{\\mathbf{K}\\mathbf{V}}}] \\in \\mathcal{R}^{T_{\\mathbf{K}\\mathbf{V}} \\times h_o}$, a sequence of vectors of length $T_{\\mathbf{K}\\mathbf{V}}$ and of another dimension $h_o$, although in general we choose $h_i = h_o$.\n",
    "\n",
    "For each query, the attention mechanism returns a convex combinations of the values $\\mathbf{V}$. The attention mechanism is defined as\n",
    "$$\n",
    "\\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathrm{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\tau} \\right) \\mathbf{V} \\ ,\n",
    "$$\n",
    "where $\\tau$ is a scaling parameter, set to $\\sqrt{h_i}$ in ([\"Attention is All You Need\", Wasrani et al. (2016)](https://arxiv.org/abs/1706.03762)). $\\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})$ is a sequence of $T_\\mathbf{Q}$ vectors, each of dimension $h_o$.\n",
    "\n",
    "The above expresson is equivalent to applying attention to each query vector $\\mathbf{q}$ separately. The output for each vector $\\mathbf{q}$ depends on the vector of weights $\\mathbf{\\Lambda} = \\mathrm{Softmax}\\left( \\frac{\\mathbf{q} \\mathbf{K}^T}{\\tau} \\right)$ with values $[\\lambda_1, \\ldots \\lambda_{T_{\\mathbf{K}\\mathbf{V}}}]$. The vector of weights $\\Lambda$ is a function of the inner-product $\\mathbf{q} \\mathbf{K}^T$, which defines a similarity metric between the the vectors $\\mathbf{q}$ and each of the key vectors $[\\mathbf{k}_1, \\ldots \\mathbf{k}_{T_{\\mathbf{K}\\mathbf{V}}}]$. Furthermore, as the weights sum to one, the output of the attention function is a convex combinations of the values:\n",
    "$$\n",
    "\\mathrm{Attention}(\\mathbf{q}, \\mathbf{K}, \\mathbf{V}) = \\sum_{i=1}^{T_{\\mathbf{K}\\mathbf{V}}} \\mathbf{\\lambda}_i \\mathbf{v}_i \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x3Z7U5IuGIJ"
   },
   "source": [
    "**Experiment** We will use the GloVe word vectors to illustrate the attention mechanism. \n",
    "\n",
    "We define queries and keys using the GloVe word vectors correspond to country names:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{Q} =& [\\mathrm{vec(\"Italy\")}, \\mathrm{vec(\"Korea\")}, \\mathrm{vec(\"Nicaragua\")}, \\ldots] \\\\\n",
    "\\mathbf{K} =& [\\mathrm{vec(\"China\")}, \\mathrm{vec(\"Russia\")}, \\mathrm{vec(\"Turkey\")}, \\ldots] \\ .\n",
    "\\end{align}\n",
    "$$\n",
    "The inner-product between paris is of country vectors will have a large values when the vectors are similar, this might happend when two countries are geographically or culturally close to each other because of the properties of the GloVe vectors. The last component required to apply the attention mechanism is set of value vectors $\\mathbf{V}$.\n",
    "\n",
    "The choice of values depends on the end problem, for this exercise we choose stay in the same theme as for the word2vec experiments. We choose the value vectors to represent the relative concept \"*capital city of a country*\", which correspond to the vector $\\mathrm{vec(\"Capital\\, city\")} - \\mathrm{vec(\"Country\")}$ in the gloVe vector space. In practice, for each country we set:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{V} =& [\\mathrm{vec(\"Beijing\")} - \\mathbf{K}_1, \\mathrm{vec(\"Moscow\")} - \\mathbf{K}_2, \\mathrm{vec(\"Ankara\")} - \\mathbf{K}_3, \\ldots] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Figure: Word vectors vec(\"capital city\") - vec(\"country\") represented in a vector space. Vectors might point in different direction depending on their position in the vector space.*\n",
    "![Attention analogies](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/attention-analogies.png?raw=1)\n",
    "\n",
    "We represent the query, key and value vectors in the GloVe vector space in the above figure. The intuition is that the value vector $\\mathrm{vec}(\\text{\"capital city of a country\"})$ is not always the same depending on the country. Therefore, in this experiment, we will attempt to extract value vectors that are contextualized on the query. To do so, we will add the output of the attention (a mixture of vectors $\\mathrm{vec}(\\text{\"capital city\"}) - \\mathrm{vec}(\\text{\"country\"})$) to the vector representation of the query (a country). This corresponds to:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec(\"Capital\\,city\\,of\\,the\\,query\\,country\")} =  \\mathrm{vec(\"Query\\,country\")} + \\underbrace{\\mathrm{Attention}(\\mathrm{vec(\"Query\\,country\")}, \\mathbf{K}, \\mathbf{V})}_{\\sum_{i=1}^{T_{\\mathbf{K}\\mathbf{V}}} \\mathbf{\\lambda}_i \\mathbf{v}_i}\n",
    "$$\n",
    "\n",
    "First, let's investigate the attention weights $\\lambda_1, \\ldots, \\lambda_{T_{\\mathbf{K}\\mathbf{V}}}$ for each query $\\mathbf{q}_1, \\ldots, \\mathbf{q}_\\mathbf{Q} $ separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee0NwTe1uGIK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# instantiate some embeddings\n",
    "embeddings = torch.nn.Embedding(*glove_vectors.shape)\n",
    "embeddings.weight.data = glove_vectors\n",
    "embeddings.weight.requires_grad = False\n",
    "\n",
    "def names_to_vectors(values: List[str]) -> torch.Tensor:\n",
    "    encodings = glove_tokenizer.encode_batch(values)\n",
    "    vectors = [embeddings(torch.tensor(e.ids)).mean(dim=0) for e in encodings]\n",
    "    return torch.stack(vectors)\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention_weights, queries_labels, keys_labels, nrows = 2):\n",
    "    \"\"\"Plot the attention scores between each of the queries and the keys.\"\"\"\n",
    "    ncols = len(attention_weights) // nrows\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize = (10,6), sharex=False, sharey=True, dpi=300)\n",
    "    colors = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    def normalize(x, values):\n",
    "        return (x -  values.min()) / (values.max() - values.min())\n",
    "    for k, ax in enumerate(axes.flat):\n",
    "        query_label = queries_labels[k]\n",
    "        attention_weights_k = attention_weights[k]\n",
    "        cols = [colors(normalize(x, attention_weights).detach().item()) for x in attention_weights_k]\n",
    "        ax.bar(keys_labels, attention_weights_k, color=cols)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        ax.set_title(f\"Query={query_label}\")\n",
    "        if k % ncols == 0 :\n",
    "            ax.set_ylabel(\"Attention score\")\n",
    "        if k >= ncols:\n",
    "            ax.set_xlabel(\"$\\mathbf{K}$\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define keys, queries and values\n",
    "queries_labels = [\"Italy\", \"Korea\", \"Nicaragua\", \"Canada\", \"Algeria\", \"India\"]\n",
    "keys_labels = [\"China\", \"Russia\", \"Turkey\", \"Japan\", \"Thailand\", \"Germany\", \"France\", \"Sweden\", \"Poland\", \"Nigeria\", \"Morocco\", \"Colombia\", \"Chile\", \"USA\", \"Pakistan\"]\n",
    "keys_cities_labels = [\"Beijing\", \"Moscow\", \"Ankara\", \"Tokyo\", \"Bangkok\", \"Berlin\", \"Paris\", \"Stockholm\", \"Warsaw\", \"Abuja\", \"Rabat\", \"Bogota\", \"Santiago\", \"Washington\", \"Islamabad\"]\n",
    "# convert to vectors\n",
    "Q = names_to_vectors(queries_labels)\n",
    "K = names_to_vectors(keys_labels)\n",
    "V = names_to_vectors(keys_cities_labels) - K\n",
    "\n",
    "\n",
    "# compute the attention weights for each query using a ´for` loop\n",
    "attention_weights = []\n",
    "for q in Q:\n",
    "    log_lambda_q = (q @ K.T) / math.sqrt(float(Q.shape[-1]))\n",
    "    attention_weights.append(log_lambda_q)\n",
    "attention_weights = torch.stack(attention_weights)\n",
    "\n",
    "# plot attention weights for each query\n",
    "plot_attention_weights(attention_weights, queries_labels, keys_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g7Jqx4suGIK"
   },
   "source": [
    "**Exercise 4**:  In this example, what is the value of $T_{\\mathbf{K}\\mathbf{V}}$, $T_{\\mathbf{Q}}$, $h_i$, $h_o$ and $\\tau$ ?\n",
    "\n",
    "> * $T_{\\mathbf{K}\\mathbf{V}} = 15$ \n",
    "> * $T_{\\mathbf{Q}} = 6$ \n",
    "> * $h_i = h_o = 300$\n",
    "> * $h_o = h_i = 300$\n",
    "> * $\\tau = 17.32$"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"T_KV\", len(K)) # T_KV\n",
    "print(\"T_Q\", len(Q)) # T_Q\n",
    "print(\"h\", Q.shape[-1])\n",
    "print(\"tau\", math.sqrt(float(Q.shape[-1])))"
   ],
   "metadata": {
    "id": "Iey1zwfg5yiL",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jIxNVJouGIL"
   },
   "source": [
    "**Implementing the attention function** We obtained a set of attention weights for each query, concatenating them results in a 2D matrix that will display bellow. Let's implement the `attention` function in the cell bellow using the inputs vectors $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ and visualize the output vector. we use [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) to implement the sum $\\sum_{i=1}^{T_{\\mathbf{K}\\mathbf{V}}} \\mathbf{\\lambda}_i \\mathbf{v}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0No1-SrduGIL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_attention_map(attention_map, queries_labels, keys_labels, print_values:bool=False, ax=None, color_bar:bool=True):\n",
    "    \"\"\"Plot the attention weights as a 2D heatmap\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = (10,6), dpi=300)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "    im = ax.imshow(attention_map, cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel(\"$\\mathbf{Q}$\")\n",
    "    ax.set_xlabel(\"$\\mathbf{K}$\")\n",
    "    ax.set_yticks(np.arange(len(queries_labels)))\n",
    "    ax.set_yticklabels(queries_labels)\n",
    "    ax.set_xticks(np.arange(len(keys_labels)))\n",
    "    ax.set_xticklabels(keys_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    if print_values:\n",
    "        for i in range(len(queries_labels)):\n",
    "            for j in range(len(keys_labels)):\n",
    "                text = ax.text(j, i, f\"{attention_map[i, j]:.2f}\",\n",
    "                            ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    if color_bar:\n",
    "      fig.colorbar(im, fraction=0.02, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def attention(Q, K, V, tau=None):\n",
    "    \"\"\"A simple parallelized attention layer\"\"\"\n",
    "    if tau is None:\n",
    "        tau = math.sqrt(float(Q.shape[-1]))\n",
    "    assert Q.shape[-1] == K.shape[-1]\n",
    "    assert K.shape[0] == V.shape[0]\n",
    "    attention_map = Q @ K.T / tau\n",
    "    attention_weights = attention_map.softmax(dim=1)\n",
    "    return torch.einsum(\"qk, kh -> qh\", attention_weights, V), attention_weights\n",
    "\n",
    "# return the output of the attention\n",
    "output, attention_weights = attention(Q, K, V, tau=1)\n",
    "\n",
    "# plot the attention weights\n",
    "plot_attention_map(attention_weights, queries_labels, keys_labels, print_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXI8BkIruGIL"
   },
   "source": [
    "**Exercise 5**:  What effect has the parameter $\\tau$ on the attention mechanism? What is happening when using a large value for $\\tau$? using a small value for $\\tau$? In the limits $\\tau \\rightarrow 0$ and $\\tau \\rightarrow \\infty$\n",
    "\n",
    "> Low values of $\\tau$ will result in a more discrete answer to the query, whereas a high value of $\\tau$ will result in a more continuous answer to the query i.e. a small $\\tau$ will most likely have one correct answer, whereas a big $\\tau$ most likely will have multiple answers.\n",
    "\n",
    "\n",
    "**Visualizing the output word vector** In the code below we use the code from the previous word2vec experiment to generate the nearest neighbour corresponding to the analogy: \n",
    "\n",
    "$$\\mathrm{vec(\"Capital\\,city\\,of\\,the\\,query\\,country\")} = \\mathrm{vec(\"Query\\,country\")} + \\mathrm{Attention}(\\mathrm{vec(\"Query\\,country\")}, \\mathbf{K}, \\mathbf{V})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aKcB64-uGIM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Report of the nearest neighbors of the vector `query + Attention(query, keys, values)``\n",
    "for i, attn_output in enumerate(output):\n",
    "    # z = query + Attention(qeury, keys, values)\n",
    "    z = Q[i] + attn_output\n",
    "    rich.print(f\"Nearest neighbors of [red]{queries_labels[i]}[/red] + [blue]Attention({queries_labels[i]}, K, V)\")\n",
    "    rich.print(vec2words(z, k=5, **glove_args, exclude_vecs=[word2vec(queries_labels[i], **glove_args)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnDCifTxuGIM"
   },
   "source": [
    "**Eercise 6** Would the results be different when shuffling the vectors $\\mathbf{K}$ and $\\mathbf{V}$ (with the same permutation for both vectors)?\n",
    "\n",
    "> The attention weights is the same, as long as the K and V are shuffled with same permutation, it is not true if shuffled randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLOpitlYuGIM"
   },
   "source": [
    "### III.b Self-attention\n",
    "\n",
    "*Figure: A self-attention layers allows attending all the sequence positions*\n",
    "![Self-attention allows attentind at all the sequence positions](https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/self-attention.png?raw=1)\n",
    "\n",
    "The attention mechanism is a transformation of a sequence of vectors $\\mathbf{Q}$ given all the values in the sequence $\\mathbf{V}$. In a self-attention layers, the attention layer is parameterized with transformations of the input sequence $\\mathbf{w}_{1:T}$ as parameters, which allows processing each vector in a sequence $\\mathbf{w}_{1:T}$ based on all the other locations. The output of the self-attention layer is a sequence of hidden states:\n",
    "$$\n",
    " \\mathbf{h}_{1:T} = \\mathrm{Attention} \\left(\\mathbf{Q}(\\mathbf{w}_{1:T}), \\mathbf{K}(\\mathbf{w}_{1:T}), \\mathbf{V}(\\mathbf{w}_{1:T}) \\right)\n",
    "$$\n",
    "\n",
    "**Illustration** Let's apply the self-attention to a list of word vectors, do you any structure emerging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gpvdqb05uGIN",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Illustration of the above using code\n",
    "hdim = 300\n",
    "sentence = \"Copenhagen Denmark Stockholm Sweden Beijing China Tokyo Japan truck car bus\"\n",
    "token_ids = torch.tensor(glove_tokenizer.encode(sentence).ids)\n",
    "tokens = [glove_vocabulary[x] for x in token_ids]\n",
    "vectors = embeddings(token_ids)\n",
    "H, attention_map = attention(vectors, vectors, vectors)\n",
    "rich.print({\n",
    "    \"Q\": Q.shape,\n",
    "    \"K\": K.shape,\n",
    "    \"V\": V.shape,\n",
    "    \"H\": H.shape\n",
    "})\n",
    "\n",
    "# visualized the log of the attention map\n",
    "plot_attention_map(attention_map.log(), tokens, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEEYKKh8uOfC"
   },
   "source": [
    "**Exercise 7**: Comment on the structure of the attention map. Why is that the case?\n",
    "\n",
    "> The structure is splitting up K/Q pairs into groups asia, scandinavia and vehicles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdrvH3USuOfM"
   },
   "source": [
    "### IV.b Masked attention\n",
    "\n",
    "The self-attention layer allows computing the hidden state $\\mathbf{h}_{t}$ based on all the input vectors $\\mathbf{w}_{1:T}$. In language modelling, we want to enforce constrains on the dependencies of $\\mathbf{h}_t$ to allow left-to-right or masked factorizations. A attention mask $\\mathbf{M} \\in \\{0, -\\infty \\}^{T \\times T}$ is of the same dimension as the matrix $ \\mathbf{Q} \\mathbf{K}^T$ and can be utilized to enforce the attention weights $\\mathbf{\\lambda}_1, \\ldots, \\mathbf{{\\lambda}_T}$ to be zero wherever it is necessary. The masked attention mechanism is expressed as\n",
    "$$\n",
    "\\mathrm{Attention} \\left(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}, \\mathbf{M} \\right) = \\mathrm{Softmax}\\left( \\mathbf{M} + \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\tau} \\right) \\mathbf{V} \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3uGReoIbqGb"
   },
   "source": [
    "**Exercise 8**:  Let's implement an attention mask corresponding to:\n",
    "1. Left-to-right language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$\n",
    "1. bidirection language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{-t})$\n",
    "1. right-to-left language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{t>})$\n",
    "\n",
    "> Answer in the code below.\n",
    "\n",
    "**NB** In the visualization bellow, we re-use the gradient map extracted from the left-to-right RNN language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z_qvaxjuGIN",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def masked_attention(Q, K, V, tau=None, mask=None):\n",
    "    \"\"\"A simple masked attention layer\"\"\"\n",
    "    if tau is None:\n",
    "        tau = math.sqrt(float(Q.shape[-1]))\n",
    "    assert Q.shape[-1] == K.shape[-1]\n",
    "    assert K.shape[0] == V.shape[0]\n",
    "    attention_map = Q @ K.T / tau\n",
    "    if mask is not None:\n",
    "        attention_map = mask + attention_map\n",
    "    attention_weights = attention_map.softmax(dim=1)\n",
    "    return torch.einsum(\"qk, kh -> qh\", attention_weights, V), attention_weights\n",
    "\n",
    "# get a more natural sentence\n",
    "sentence = \"Masked attention allows implementing dependency constrains between inputs and outputs.\"\n",
    "token_ids = torch.tensor(glove_tokenizer.encode(sentence).ids)\n",
    "tokens = [glove_vocabulary[x] for x in token_ids]\n",
    "vectors = embeddings(token_ids)\n",
    "\n",
    "# EXERCISE: Implement the masks corresponding to each factorization\n",
    "# Hint: check the documentation for `torch.diag()`, `torch.triu()` and `torch.tril()`\n",
    "T = len(token_ids)\n",
    "p_ws1 = torch.distributions.Categorical(logits=attention_map)\n",
    "masks = {\n",
    "    \"left-to-right\": torch.triu(p_ws1.logits, 1), # <- YOUR CODE HERE\n",
    "    \"bidirectional\": torch.diag(torch.diag(p_ws1.logits,0)), # <- YOUR CODE HERE\n",
    "    \"right-to-left\": torch.tril(p_ws1.logits, -1), # <- YOUR CODE HERE\n",
    "}\n",
    "for key in masks.keys():\n",
    "    if masks[key] is not None:\n",
    "        masks[key] = masks[key] = torch.where(masks[key] == 0, 0, -math.inf)\n",
    "\n",
    "# visualized the log of the masked attention map\n",
    "fig, axes = plt.subplots(ncols=1+len(masks), figsize = (16,6), sharex=False, sharey=False, dpi=300)\n",
    "# plot the gradient map from the RNN LM\n",
    "axes.flat[0].imshow(grad_magnitude, sns.color_palette(\"viridis\", as_cmap=True))\n",
    "axes.flat[0].set_xlabel(\"$t$ (input)\")\n",
    "axes.flat[0].set_ylabel(\"$t'$ (output)\")\n",
    "axes.flat[0].grid(False)\n",
    "axes.flat[0].set_title(\"gradient map (RNN LM)\")\n",
    "# plot the attention map\n",
    "for ax, (mask_name, mask) in zip(axes.flat[1:], masks.items()):\n",
    "    if mask is not None:\n",
    "        H, attention_map_masked = masked_attention(vectors, vectors, vectors, mask=mask)\n",
    "        plot_attention_map(attention_map_masked.log(), tokens, tokens, ax=ax, color_bar=False)\n",
    "    ax.set_title(f\"Attention map {mask_name}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwuFdsZQuGIO"
   },
   "source": [
    "----\n",
    "## IV. Transformers\n",
    "\n",
    "<img src=\"https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/transformer.png?raw=1\" alt=\"Transformer architecture\" width=\"600\"/>\n",
    "\n",
    "In this section we are going to introduce the [Transformer (\"Attention is all you need\", Vaswani (2017))](https://arxiv.org/abs/1706.03762) architecture.\n",
    "\n",
    "For further information, see the excellent PyTorch tutorial [\"language translation using Transformers\"](https://pytorch.org/tutorials/beginner/translation_transformer.html) and blog article [\"Annotated Transformer\"](https://nlp.seas.harvard.edu/2018/04/03/attention.html), which review the original in great details and provide additional content such as visualizations of the learned attention maps.\n",
    "\n",
    "**Architecture** A Transformer is composed of two main components: a decoder which implements a language model and an encoder. The encoder is only required for conditional language models like those used in translation tasks. Each of the two components is made by stacking Transformer layers (layers with and without conditioning). Each layer transforms a sequence of hidden state $\\mathbf{h}_{1:T}^l$ into another sequence $\\mathbf{h}_{1:T}^{l+1}$. The input tokens are converted into the first state $\\mathbf{h}_{1:T}^0$ using an embedding layer coupled with positioal encodings. the last hidden state $\\mathbf{h}_{1:T}^{L}$ is projected into the vocabulary space using a liner layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5P9m0_PuGIO"
   },
   "source": [
    "### IV.a Positional encodings\n",
    "\n",
    "The position of a word in the sentence can make a difference in the type of meaning that word is assuming. In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. While an RNN see the tokens sequntially and therefore can get an idea of the position of the different words in the sentence, a Transformer model is not processing the input sequentially, therefore this model needs a way to understand which part of the sentence it is processing.\n",
    "\n",
    "The positional encoding should satisy the following properties:\n",
    "- for each time-step, i.e. word position in a sentence, it should produce a unique vector;\n",
    "- the distance between any two time-steps should be consistent;\n",
    "- it should be deterministic.\n",
    "\n",
    "In the paper they proposed to use a positional encoding that produce a vector of size $d_{\\text{model}}$ as the embedding size, so we can add them together. The method proposed by the paper can be defined as:\n",
    "\n",
    "\\begin{align}\n",
    " PE(t,2i) = \\sin{\\left(\\frac{t}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)} \\quad\\text{and}\\quad PE(t,2i+1) = \\cos{\\left(\\frac{t}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)}\n",
    "\\end{align}\n",
    "\n",
    "where $t$ is the position of the world in the sentence and $i$ is the index in the embedding. To do this we have to consider $i=0,2,4,\\dots$. Indeed, then for each $i$ we compute both the functions above and thertefore $2i$ and $2i+1$ represents the column index of the positional encoding vector. In other words, for each even index $i$ we apply the first equation and for every odd index we apply the second one. Therefore, each dimension of the positional encoding corresponds to a sinusoid and the vector that we will obtain will have the following form:\n",
    "\n",
    "$$ PE = [ \\sin{(t)}, \\cos{(t)}, \\sin{\\left(\\frac{t}{10000^{\\frac{2}{d_{\\text{model}}}}}\\right)}, \\cos{\\left(\\frac{t}{10000^{\\frac{2}{d_{\\text{model}}}}}\\right)}, \\cdots]$$\n",
    "\n",
    "\n",
    "The class defining the Positional Encoding is given by this code snippet. We can also try to plot and look at the positional encoding we get, this is done in the second code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Enprc5RiuGIO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# let's assume we havee a sequence of length 10 and embedding of length 128\n",
    "# how their positional encoding looks like\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# we define the positional encoding for a 128-dimensional vector and for this \n",
    "# example we do not use dropout\n",
    "pe = PositionalEncoding(64, dropout=0, max_len=1000)\n",
    "\n",
    "# note here we are assuming our input to be\n",
    "# batch_size x sequence_length x d_model\n",
    "# but in pytorch they are assuming \n",
    "# sequence_length x batch_size x d_model\n",
    "# we also assume zeros so we can see how the positional encoding looks like\n",
    "x_inputs = torch.zeros(1, 512, 64)\n",
    "y = pe(x_inputs)\n",
    "\n",
    "sns.heatmap(y.squeeze(0).T, cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "plt.ylabel('Dimension')\n",
    "plt.xlabel('Position in the sentence')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# in the figure below each row corresponds to the vector we are adding \n",
    "# to our embedding vector when the word is at that position in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoqePMoouGIP"
   },
   "source": [
    "### IV.b Base layers\n",
    "\n",
    "*Figure: (Left) A layer of base Transformer layer with three components: multi-head attention layer, feed-forward layer, and add & norm layer. \n",
    "(Right) Multi-head attention layer.*\n",
    "\n",
    "<img src=\"https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/transformer-layer.png?raw=1\" alt=\"base Transformer layer (without conditioning)\" width=\"200\" style=\"margin-right: 100px\"/>\n",
    "<img src=\"https://github.com/mikk5829/02456-deep-learning-with-PyTorch/blob/mikkel/5_Transformers/images/multi-head-attention.png?raw=1\" alt=\"Multi-head attention\" width=\"200\"/>\n",
    "\n",
    "Each base layer of index $l$ takes a sequence of hidden state $\\mathbf{h}_{1:T}^l$ as input, and output another sequence $\\mathbf{h}_{1:T}^{l+1}$.\n",
    "\n",
    "**Multi-head attention** The attention mechanism introduced in the previous section depends on a softmax of inner-products, which might be sparse depending on the value of the vectors, in that case, the layer can only attend to a few positions in the sequence. To enable attending to more positions in the input sentence, multiple attention mechanism can be used in parallel. This is what we call a multi-head attention layer:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{MultiHeadAttention} \\left(\\mathbf{Q}^{1:P}, \\mathbf{K^{1:P}}, \\mathbf{V}^{1:P}, \\mathbf{M} \\right) = [ \\mathrm{Attention} \\left(\\mathbf{Q}^{1}, \\mathbf{K^{1}}, \\mathbf{V}^{1}, \\mathbf{P} \\right), \n",
    "\\ldots\n",
    "\\mathrm{Attention} \\left(\\mathbf{Q}^{P}, \\mathbf{K^{P}}, \\mathbf{V}^{P}, \\mathbf{M} \\right)] \\ ,\n",
    "\\end{align}\n",
    "$$\n",
    "where each set of vectors $\\mathbf{Q}^{i}, \\mathbf{Q}^{i}, \\mathbf{Q}^{i}$ corresponding to the head index $i$ is obtained using a separate linear transformation of the input sequence.\n",
    "\n",
    "**Feed-forward** The multi-head attention layer allows looking up multiple positions of the input sequence, but the output is only a linear combination of the value vector $\\mathbf{V}$. A multi-layer neural network (feed-forward layer) is applied **element-wise** to each element of the sequence of hidden states to allow modelling more complex non-linear dependencies.\n",
    "\n",
    "**Add & Norm** A Transformer is a deep neural network, and therefore might be difficult to optimize. Similarly deep neural networks in the image processing field, Transformer layer rely on two stabilizing components:\n",
    "1. [Residual connections](https://arxiv.org/abs/1512.03385) allow to bypass the attention layer as well as the feed-forward layer.\n",
    "2. [Layer normalization](https://arxiv.org/abs/1607.06450): allow enforcing that the output of a Transformer layer has values that are properly distributed\n",
    "\n",
    "**Implementation:** Here is an implementation for the attention and multi-head attention. You can also check the official [PyTorch implementation](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDOhBgQSuGIP",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -math.inf)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z_vEKJFuGIQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"A simple Multi-head attention layer.\"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.attn = None # store the attention maps\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        nbatches = query.size(0)\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZiGQ6I3uGIQ"
   },
   "source": [
    "### IV.c Conditional layers\n",
    "\n",
    "In the description of the base layer, we have applied attention to the input sequence (*self-attention*). Machine translation is a sequence-to-sequence task, which requires and *encoder* component, that encodes a source text into a sequence of hidden states $\\mathbf{g_{1:T'}}$. The base layer can be modified with an additional attention layer that is conditionned on the source text. Given a sequence of hidden states $\\mathbf{h}_{1:T}$, the conditional attention layer is:\n",
    "$$\n",
    "\\mathrm{Attention}(\\mathbf{Q}(\\mathbf{h}_{1:T}), \\mathbf{K}(\\mathbf{g}_{1:T'}), \\mathbf{V}(\\mathbf{g}_{1:T'})) \\ .\n",
    "$$\n",
    "Conditional attention layers are place right after the self-attention layers, before the feed-forward layer (see diagram).\n",
    "\n",
    "### IV.d Pre-training as language models\n",
    "\n",
    "Transformers (the decoder component) are language models can therefore be pre-train on vast amount of unlabelled text via maximum likelihood or pseudo likelihood. They allow obtaining contextualized text representations, that can be applied to a multitude of downstream tasks (question-answering, classification, ...). While the original Transformer architecture was applied to a sequence-to-sequence problem with a component that encodes the source text and a language model decoder conditioned on the encoded source text. Transformer-based language models consist in a single decoder component (without conditional attention). The two main alternatives for language modelling are:\n",
    "\n",
    "* [Generative Pre-trained Transformers (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf): autoregressive left-to-right language models implemented using a Transformer.\n",
    "\n",
    "* [Bidirectional Encoder Representations from Transformers (BERT)](https://arxiv.org/abs/1810.04805): a masked language model trained to predict tokens that are randomly masked out (masked language model) and trained to predict whether two sentences are related or not (next-sentence prediction (NSP) task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJhmp8kJuGIQ"
   },
   "source": [
    "___\n",
    "## V. Applications of Transformer-based language models\n",
    "\n",
    "Let's experiment with a few pre-trained Transformers using the [🤗 HuggingFace](https://huggingface.co/) environment and the [OpenAI API](https://beta.openai.com/docs/guides/completion). \n",
    "\n",
    "HuggingFace is a company that manages multiple open source projects:\n",
    "* **[tokenizers](https://huggingface.co/docs/tokenizers/index)**: a very efficient implementation of tokenizers using Rust.\n",
    "* **[datasets](https://huggingface.co/docs/datasets/index)**:a library for handling and sharing large datasets (in particular they use [Apache Arrow](https://arrow.apache.org/)  for efficient data loading from disk)\n",
    "* **[transformers](https://huggingface.co/docs/transformers/index)**: implementation and sharing of Transformers in Tensorflow, PyTorch and JAX\n",
    "\n",
    "All HuggingFace models and datasets (text, audio, image and more) can be accessed through the [🤗 Hub](https://huggingface.co/), and many models can tested lives on [🤗 spaces](https://huggingface.co/spaces). In the examples bellow, we will first try to manipulate data and models using lower primitives (tokenizing data, loading a model, generating / inference), so you can interact if the intermediate variables if you want to. Then we will us the blackbox [`Pipeline`](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/pipelines#transformers.pipeline) object. If you want to apply Transformers without modifying any of the components, the `Pipeline` can be used to perform complex tasks in one line of code, as showed here with the translation task.\n",
    "\n",
    "The OpenAI API gives access to GPT-3 ([\"Language Models are Few-Shot Learners\", Brown et al. (2020)](https://arxiv.org/abs/2005.14165)) through a [playground](https://beta.openai.com/playground), where you can test the text completion capabilities of these models. GPT-3 is a large language model (up to 175 billion parameters) which has acquired impressive language understanding capabilities. It can be applied to solve new tasks without task-specific fine-tuning. [OpenAI gives you $18 to of API credits, but careful with the number of calls: running the largest version of GPT-3 (´davinci´) can be expensive](https://openai.com/api/pricing/).\n",
    "\n",
    "### V.a Language generation\n",
    "\n",
    "Let's experiment with GPT-2 (in the notebook, we use the smaller [`distilgpt2`](https://huggingface.co/distilgpt2), but feel free to use the original `gpt2` if you have enough compute)\n",
    "\n",
    "**Experiment** Generate text using GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axCm-cueuGIR",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load a GPT-2 model\n",
    "model_id = \"distilgpt2\"\n",
    "# model_id = \"gpt2\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModel.from_pretrained(model_id)\n",
    "# Here we want to load the weights of GPT2 as an autoregressive LM, or \"causal\" LM: we use the class `AutoModelForCausalLM`.\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uug5lB4auGIR",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "prompt = tokenizer.bos_token # use the Begining Of Sentence token to initialize allow generating text from scratch\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# expand input_ids, so we can sample multiple generations in parallel\n",
    "input_ids = input_ids.repeat(5, 1)\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "# documentation for `model.generate()` https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "output = model.generate(input_ids, do_sample=True, temperature=1., max_length=50)\n",
    "decoded = tokenizer.batch_decode(output)\n",
    "for txt in decoded:\n",
    "    txt = txt.replace(\"\\n\", \"\")\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBJNlOKRuGIR"
   },
   "source": [
    "**Exercise 9**: Play with the temperature parameter, what is it controlling? how can you related it to the definition of a language model?\n",
    "\n",
    "> The GPT-2 model is more confident but also more conservative when temperature is modest (for example, 0,2), and more diverse but also more error-prone when temperature is large (for example, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxf9whBIuGIS"
   },
   "source": [
    "### V.b. Prompt-based learning\n",
    "\n",
    "Prompt-based learning consists in framing a problem into a *prompt* which completion by a language model corresponds to the answer. In other words, it consists in using natural language to interface with language models.\n",
    "\n",
    "**Experiment** Do language models know what hygge is?\n",
    "\n",
    "**Exercise 10**: Write a prompt that triggers a language model to define \"hygge\".\n",
    "\n",
    "> *write your prompt here and test it using GPT-2 and GPT-3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyOWrcVIuGIS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Hygge means \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "input_ids = input_ids.repeat(5, 1)\n",
    "output = model.generate(input_ids, do_sample=True, temperature=0.8, max_length=50)\n",
    "decoded = tokenizer.batch_decode(output)\n",
    "for txt in decoded:\n",
    "    txt = txt.replace(\"\\n\", \"\")\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4j2-kvvuGIS"
   },
   "source": [
    "**Exercise 11**: Test your prompt [with GPT-3](https://beta.openai.com/playground)\n",
    "\n",
    "> What is hygge?\n",
    "\n",
    "Hygge is a Danish and Norwegian word for a mood of coziness and comfortable conviviality with feelings of wellness and contentment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9jvbMrWtKcj"
   },
   "source": [
    "\n",
    "**More prompt-based learning** Large language models can answer complex questions. In [\"Can large language models reason about medical questions? Liévin et al. (2022)\"](https://arxiv.org/abs/2207.08143), we observed that GPT-3 can in many cases solve complex medical problems which require 1) reading and understaing the provided medical cases, 2) retrieving expert knowledge (implicitely stored in the weights) and 3) detail a multi-step reasoning to reach a conclusion. Multi-step reasoning can be triggered using zero-shot chain-of-thought prompting ([\"Large Language Models are Zero-Shot Reasoners\", Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)), which boils down to prompting a language model with \"*Let's think step by step*\". \n",
    "\n",
    "You can experiment with an example of chain-of-thought prompt using [this template](https://beta.openai.com/playground/p/ka3F37wLN57m62YkYjvyDx0N?model=text-davinci-002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-dT_fV-uGIS"
   },
   "source": [
    "### V.c Language Translation\n",
    "\n",
    "**Experiment** Let's use the `Pipeline` object to translate French to English in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jV1Xy5SuGIT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fr_en_translator = transformers.pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "output = fr_en_translator([\"Mon cheval est très rapide\", \"Omelette du fromage\"])\n",
    "rich.print(output)\n",
    "# del fr_en_translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQKU7HkFuGIT"
   },
   "source": [
    "### V.d Information Retrieval\n",
    "\n",
    "BERT allows converting pieces of text (sentences, paragraphs or even documents) into vectors. [DPR (\"Dense Passage Retrieval for Open-Domain Question Answering\", Karpukhin et al. (2020))](https://arxiv.org/abs/2004.04906) is a BERT model fine-tuned for the task of information retrieval. DPR converts documents and questions into vectors fixed-size vectors which can be compared via inner product. It is then possible to index large collections of documents (Wikipedia) efficiently using maximum inner product search libraries like [`faiss`](https://faiss.ai).\n",
    "\n",
    "**Experiment** Let's implement a simple search engine using a pre-trained DPR. In the same vein as the previous experiments, we define each document as a short factual sentences \"*`<city>` is the capital city of `<country>`*\". The questions are set to \"*What is the capital city of `<country>`?*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjbZ-qd4uGIT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModel.from_pretrained(model_id).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygptaSOmuGIU",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define some documents and questions\n",
    "documents = [f\"{city} is the capital of {country}\" for city, country in zip(keys_cities_labels, keys_labels)]\n",
    "questions =  [f\"What is the capital city of {country}?\" for country in keys_cities_labels]\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer_args = {\"padding\":True, \"truncation\":True, \"return_tensors\": \"pt\"}\n",
    "documents_ = tokenizer(documents, **tokenizer_args)\n",
    "questions_ = tokenizer(questions, **tokenizer_args)\n",
    "\n",
    "# Compute the question and document vectors\n",
    "with torch.no_grad():\n",
    "    document_vectors = model(**documents_).pooler_output\n",
    "    question_vectors = model(**questions_).pooler_output\n",
    "\n",
    "# compute the inner product (attention map) between all pairs of questions and documents\n",
    "attention_map = question_vectors @ document_vectors.T\n",
    "attention_map = attention_map.softmax(dim=1)\n",
    "\n",
    "# plot as an attention map\n",
    "plot_attention_map(attention_map, questions, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ail6ypmsFdO8"
   },
   "source": [
    "**Exercise 12**: Is DPR retrieving the right document for each question? Comment on the shape of the attention map.\n",
    "\n",
    "> *Insert your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGRt_VQDuGIU"
   },
   "source": [
    "---\n",
    "## Credits and additional references\n",
    "\n",
    "### Credits\n",
    "\n",
    "Written by Valentin Liévin and reusing parts of the [Notebook](https://github.com/DeepLearningDTU/Deep-Learning-with-PyTorch-1weekcourse/tree/master/3_Recurrent/notebook_transformer) from Frederico Bergamin.\n",
    "\n",
    "\n",
    "### Additional references\n",
    "\n",
    "1. RNNs.\n",
    "- [Blog post: \"The Unreasonable Effectiveness of Recurrent Neural Networks\" (Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [lecture: \"Language Models and RNNs\" (Stanford)](https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).\n",
    "\n",
    "2. Transformers\n",
    "- [Video from the main author of the paper](https://www.youtube.com/watch?v=rBCqOTEfxvg&t=894s)\n",
    "- [Video lecture from Deepmind](https://www.youtube.com/watch?v=8zAP2qWAsKg&t=2073s)\n",
    "- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Blog Post on the Positional Encoder](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [Transformer from Scratch](https://e2eml.school/transformers.html#positional_encoding)\n",
    "- [Another blog post explaining transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "### Re-using the images\n",
    "\n",
    "[Figma design file](https://www.figma.com/file/d7BHs0yMhM9CTer43mrTe5/deep-learning-course?node-id=0%3A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib3AVsyguGIU"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "e72cf85b1cff461a3287147b79b2cfd4bf5f57e721dfc08300645559174c8f4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c0ac4f025e3e4f1f8139f32b6944c4eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c24c3519f3d4b9fb74212e04c295375",
       "IPY_MODEL_29e1fba88dc44a7490fc65cd9dc0eef1",
       "IPY_MODEL_d3b630f54521489fa7da02ec47e9e220"
      ],
      "layout": "IPY_MODEL_11a608d17f0041d7966e54b1379eb963"
     }
    },
    "2c24c3519f3d4b9fb74212e04c295375": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c4b9c55c6ac4582bc98acdbe774c87c",
      "placeholder": "​",
      "style": "IPY_MODEL_e5fe5c0a9bbe4a41a4f56931bbafb41b",
      "value": "Downloading:  80%"
     }
    },
    "29e1fba88dc44a7490fc65cd9dc0eef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a3afe212a70491d8dacfc4ff7ba45e4",
      "max": 862182753,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_971ddad710c6403db026dfe89176cec5",
      "value": 691378176
     }
    },
    "d3b630f54521489fa7da02ec47e9e220": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_661e8b0c7ab6495f93102bb2b0a23444",
      "placeholder": "​",
      "style": "IPY_MODEL_2e2f47bcd0294757b30a734ea11fa959",
      "value": " 691M/862M [00:29&lt;00:08, 19.3MB/s]"
     }
    },
    "11a608d17f0041d7966e54b1379eb963": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4b9c55c6ac4582bc98acdbe774c87c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5fe5c0a9bbe4a41a4f56931bbafb41b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a3afe212a70491d8dacfc4ff7ba45e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "971ddad710c6403db026dfe89176cec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "661e8b0c7ab6495f93102bb2b0a23444": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e2f47bcd0294757b30a734ea11fa959": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}